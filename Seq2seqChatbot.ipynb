{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence-to-sequence Chatbot \n",
    "A Simple Explanation about Sequence-to-sequence Chatbot Using GRU and Luong Attention Mechanism <br>\n",
    "\n",
    "\n",
    "Before we dive into CODE, first we need to understand how this Sequence-to-sequence architecture and all of the component Works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence-to-sequence (Seq2seq) Explanation \n",
    "Base on it name, seq2seq works with sequence data (time series, sentence etc.) and trying to continue those sequence with another related sequence.\n",
    "\n",
    "<img src='/Asset/Example1.jpg'>\n",
    "![](Asset/Example1.jpg)\n",
    "\n",
    "As we can see, the input sentences \"I want to drink Water\" is consist of words and the seq2seq give an result of another collection of words in a sentences \"Because I am thirsty\"\n",
    "\n",
    "## Seq2seq Architecture\n",
    "To make it easier to understand about how this seq2seq works, this is a seq2seq architecture\n",
    "\n",
    "<img src='/Asset/Seq2seqArchitecture.jpg'>\n",
    "<!-- ![](Asset/Seq2seqArchitecture.jpg) -->\n",
    " \n",
    "Base on it structure, seq2seq contain 2 parts named Encoder and Decoder. Encoder works to ENCODE the input sequence, the result of this ENCODED process is a context of input sequence. Next, This context of input sequence entered into Decoder. Then, Decoder will process this context to predicit the next word for a certain iteration.\n",
    "\n",
    "To make those process happend, each Encoder and Decoder contain set of neural network layer. Basically, we can customized each layer by on our own. For this tutorial we gonna use this layer formation:<br>\n",
    "\n",
    "ENCODER:\n",
    "- <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\" >Embedding</a>\n",
    "- <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.GRU.html\" >Bidirectional Gated Recurrent Unit</a>\n",
    "\n",
    "DECODER:\n",
    "- Embedding\n",
    "- Vanilla Gated Recurrent Unit\n",
    "- <a href=\"https://arxiv.org/abs/1508.04025\" >Luong Attention Mechanism</a> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3a4afe6c112a183bfbb4cc731ce1483c0c2d5da4fb79320f7f523ddeb254817c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 ('chatbot')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
